{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c68dbf1-ee0a-459b-a7a6-70a262591075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in X: 0\n",
      "Missing values in y: 0\n",
      "X_train shape: (900, 384)\n",
      "y_train shape: (900,)\n",
      "X_train: [[ 0.43065925  0.31165501 -1.17599463 ... -0.0034474  -1.24896036\n",
      "  -0.46903949]\n",
      " [-0.42325022 -1.61802092  1.47134977 ... -0.83380362  2.60301726\n",
      "   0.16690703]\n",
      " [ 0.41468977  0.41584676  0.75013947 ... -0.96415061 -0.31073781\n",
      "  -0.14098914]\n",
      " ...\n",
      " [ 0.38083368  0.31547139  0.92295139 ... -0.6160675  -0.413935\n",
      "   0.99635437]\n",
      " [ 0.54531332 -1.01878535 -0.41128906 ... -0.34846709  0.40775728\n",
      "  -0.97471566]\n",
      " [-0.77235995 -0.54224716  0.24817542 ... -0.42302967  1.2995227\n",
      "  -1.22055318]]\n",
      "y_train: [4 3 3 3 3 2 3 4 3 5 2 5 3 4 4 4 4 3 3 2 2 2 2 2 0 0 3 5 4 2 2 2 3 2 5 3 2\n",
      " 4 2 3 4 4 2 2 0 2 1 1 1 3 2 1 5 2 1 3 2 4 2 3 4 2 3 4 4 3 4 3 4 2 1 3 1 3\n",
      " 4 4 2 3 2 2 2 3 4 1 2 3 3 3 2 4 2 2 3 3 4 1 3 3 4 3 4 4 0 2 2 2 4 2 2 4 3\n",
      " 2 4 4 3 1 2 3 5 2 2 4 2 2 2 2 2 3 2 2 4 5 2 3 3 1 1 3 0 5 2 2 3 4 4 2 1 1\n",
      " 2 1 5 3 5 4 4 3 3 2 3 0 2 4 3 1 1 4 4 2 3 3 1 2 4 2 3 2 3 2 4 3 3 1 3 3 2\n",
      " 5 2 3 0 3 3 2 3 2 1 2 3 4 4 3 3 2 4 2 3 4 2 4 3 2 4 4 3 4 2 5 3 1 3 5 3 5\n",
      " 0 2 2 4 2 3 2 2 3 4 4 4 3 3 3 4 3 3 5 4 3 4 5 2 3 4 5 2 2 3 4 2 4 1 2 2 3\n",
      " 3 3 3 1 3 1 3 3 3 2 2 3 3 3 5 5 4 0 2 2 2 2 4 2 4 3 4 4 2 2 1 3 1 5 2 3 4\n",
      " 2 1 1 2 3 2 4 3 3 3 3 2 4 2 0 3 2 3 2 3 2 3 3 4 3 1 4 2 2 4 1 3 4 2 1 1 3\n",
      " 3 4 3 2 0 4 4 3 4 2 3 4 2 2 2 3 4 2 2 2 0 3 4 1 2 4 2 3 2 4 3 3 3 4 3 2 2\n",
      " 4 4 2 2 1 4 3 3 1 2 3 3 1 5 3 3 4 3 0 2 3 4 1 2 2 3 4 3 2 3 4 5 4 3 2 2 0\n",
      " 3 2 4 4 2 2 1 3 0 2 4 4 3 4 2 2 3 2 2 3 1 3 1 4 2 4 2 3 2 1 4 3 2 2 2 2 3\n",
      " 5 3 1 4 4 3 3 4 4 3 1 1 2 5 1 3 5 3 3 4 1 3 2 2 3 1 2 2 1 0 4 4 4 2 4 2 4\n",
      " 4 4 2 2 4 2 2 0 4 4 3 3 1 2 5 2 2 3 2 3 2 0 3 4 4 4 2 3 4 5 0 2 2 4 4 4 3\n",
      " 2 3 3 2 3 3 3 3 4 3 0 0 2 2 4 5 2 1 5 3 3 2 2 4 4 4 4 4 4 3 2 1 2 2 4 2 0\n",
      " 3 1 2 3 3 4 4 2 2 4 4 2 0 4 5 2 5 2 2 0 3 3 5 2 2 5 3 0 4 4 2 4 4 3 5 5 4\n",
      " 3 2 3 5 2 4 3 4 2 4 4 4 1 2 4 1 2 2 4 3 4 3 1 1 2 2 5 2 5 3 4 4 0 4 0 2 3\n",
      " 2 2 4 3 3 4 4 4 2 3 2 2 2 4 3 3 5 5 3 4 3 4 3 4 2 2 4 2 4 4 3 3 2 2 2 2 1\n",
      " 4 3 2 3 5 2 3 4 4 2 2 4 2 1 2 2 2 4 2 2 4 4 5 4 1 1 3 2 4 5 3 4 5 2 3 0 4\n",
      " 5 1 3 4 4 2 5 4 3 2 4 2 3 4 1 3 2 2 3 3 2 4 3 4 2 4 4 3 2 3 2 5 3 2 2 3 3\n",
      " 3 4 2 2 2 2 5 4 2 2 3 3 5 5 3 2 3 3 0 4 3 4 3 3 3 3 3 1 2 3 4 2 3 0 3 2 3\n",
      " 3 5 4 3 3 4 1 3 2 3 4 3 3 2 4 3 4 4 2 2 3 4 2 3 4 4 3 3 4 3 3 1 4 2 3 4 3\n",
      " 2 0 1 4 2 2 2 2 2 3 3 3 1 0 3 2 2 3 2 3 4 2 3 2 4 2 4 2 4 2 2 3 2 3 3 4 4\n",
      " 2 0 3 3 5 1 4 3 3 2 0 2 3 2 2 4 0 3 4 3 5 3 4 2 4 3 3 5 2 4 3 2 0 2 2 4 2\n",
      " 4 2 3 1 4 1 2 2 1 4 5 4]\n",
      "Best Perceptron Params: {'penalty': 'l2', 'max_iter': 1000, 'eta0': 0.001291549665014884}\n",
      "Perceptron Test Score: 0.4247787610619469\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset from Excel file\n",
    "def load_data(file_path, sheet_name=0):\n",
    "    data = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    print(f\"Data loaded successfully with shape: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# Hyperparameter tuning for Perceptron\n",
    "def tune_perceptron(X_train, y_train):\n",
    "    perceptron = Perceptron()\n",
    "    param_dist = {\n",
    "        'penalty': ['l2', None],  # Valid options for regularization\n",
    "        'eta0': np.logspace(-4, 1, 10),  # Learning rate values\n",
    "        'max_iter': [1000, 2000, 3000]\n",
    "    }\n",
    "    random_search = RandomizedSearchCV(perceptron, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    return random_search\n",
    "\n",
    "# Hyperparameter tuning for MLP\n",
    "def tune_mlp(X_train, y_train):\n",
    "    mlp = MLPClassifier()\n",
    "    param_dist = {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'alpha': np.logspace(-4, 1, 10),\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    }\n",
    "    random_search = RandomizedSearchCV(mlp, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    return random_search\n",
    "\n",
    "# Load data\n",
    "data = pd.read_excel('training_mathbert 3.xlsx')\n",
    "X = data.iloc[:, :-1].values  # Features\n",
    "y = data.iloc[:, -1].values   # Target\n",
    "\n",
    "# Ensure target labels are integers\n",
    "y = y.astype(int)\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"Missing values in X: {np.isnan(X).sum()}\")\n",
    "print(f\"Missing values in y: {np.isnan(y).sum()}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Debugging: Check shape and initial values\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_train:\", X_train[:1000])\n",
    "print(\"y_train:\", y_train[:1000])\n",
    "\n",
    "# Tune Perceptron\n",
    "perceptron_search = tune_perceptron(X_train, y_train)\n",
    "print(f\"Best Perceptron Params: {perceptron_search.best_params_}\")\n",
    "print(f\"Perceptron Test Score: {perceptron_search.score(X_test, y_test)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3b99a69-7c59-419e-804c-816044b08827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1126, 384)\n",
      "y shape: (1126,)\n",
      "X: [[-0.08992592  0.34387389  0.17638168 ...  0.10559644  0.1964383\n",
      "   0.11719853]\n",
      " [ 0.30326107  0.08492954  0.04736905 ... -0.02969474  0.33597666\n",
      "  -0.19753899]\n",
      " [-0.27429068  0.21680066  0.02911038 ... -0.08576754  0.5129559\n",
      "   0.02333383]\n",
      " ...\n",
      " [-0.29696792 -0.20464683  0.07746858 ...  0.11653966  0.19947569\n",
      "  -0.09766734]\n",
      " [-0.07764415  0.16842389 -0.18984014 ... -0.16399319  0.06891123\n",
      "   0.0273101 ]\n",
      " [-0.22067161  0.30338642  0.34436339 ...  0.18375197  0.11251207\n",
      "  -0.08536078]]\n",
      "y: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.25       0.25\n",
      " 0.375      0.5        0.5        0.5        0.5        0.5\n",
      " 0.5        0.5        0.5        0.5        0.5        0.5\n",
      " 0.5        0.5        0.5        0.5        0.5        0.5\n",
      " 0.5        0.5        0.625      0.75       0.75       0.75\n",
      " 0.75       0.875      0.875      1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.125      1.125      1.125      1.125      1.125\n",
      " 1.15625    1.1875     1.25       1.25       1.25       1.25\n",
      " 1.25       1.25       1.25       1.25       1.25       1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.5\n",
      " 1.5        1.5        1.5        1.5        1.5        1.5\n",
      " 1.5        1.5        1.5        1.5        1.5        1.5\n",
      " 1.5        1.5        1.5        1.5        1.5        1.5625\n",
      " 1.5625     1.5625     1.625      1.625      1.65       1.75\n",
      " 1.75       1.75       1.75       1.75       1.75       1.75\n",
      " 1.75       1.75       1.75       1.75       0.5        1.\n",
      " 1.         0.5        1.         1.75       1.75       1.75\n",
      " 1.         1.         0.5        1.875      1.875      1.875\n",
      " 1.875      1.5        1.875      1.875      2.         2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.125      2.125      2.125      2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.         2.         2.25       2.25\n",
      " 2.25       2.25       2.25       2.25       2.25       2.25\n",
      " 2.25       2.25       2.25       2.25       2.25       2.25\n",
      " 2.25       2.25       2.25       2.25       2.25       2.25\n",
      " 2.25       2.25       2.25       2.25       2.25       2.25\n",
      " 2.25       2.25       2.25       2.25       2.25       2.25\n",
      " 2.25       2.25       2.25       2.25       2.25       2.\n",
      " 2.25       2.25       2.25       2.25       2.25       2.25\n",
      " 2.25       2.25       2.25       2.25       2.25       2.25\n",
      " 2.25       2.25       2.25       2.25       2.25       2.25\n",
      " 2.25       2.25       2.25       2.3125     2.3125     2.33333333\n",
      " 2.33333333 2.33333333 2.375      2.375      2.375      2.375\n",
      " 2.375      2.375      2.375      2.375      2.375      2.375\n",
      " 2.375      2.375      2.375      2.375      2.375      2.375\n",
      " 2.375      2.375      2.375      2.375      2.375      2.375\n",
      " 2.375      2.375      2.375      2.375      2.375      2.375\n",
      " 2.375      2.4375     2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.54166667 2.5625     2.625      2.625      2.625      2.625\n",
      " 2.625      2.625      2.625      2.625      2.625      2.625\n",
      " 2.625      2.625      2.625      2.625      2.625      2.625\n",
      " 2.625      2.625      2.625      2.625      2.625      2.625\n",
      " 2.625      2.625      2.625      2.625      2.625      2.625\n",
      " 2.625      2.625      2.625      2.625      2.625      2.65625\n",
      " 2.66071429 2.66666667 2.6875     2.75       2.75       2.75\n",
      " 2.75       2.75       2.75       2.75       2.75       2.75\n",
      " 2.75       2.75       2.75       2.75       2.75       2.75\n",
      " 2.75       2.75       2.75       2.75       2.75       2.75\n",
      " 2.75       2.75       2.75       2.75       2.75       2.75\n",
      " 2.75       2.75       2.75       2.75       2.75       2.75\n",
      " 2.75       2.75       2.75       2.75       2.75       2.75\n",
      " 2.75       2.75       2.75       2.75       2.75       2.75\n",
      " 2.75       2.75       2.75       2.75       2.75       2.875\n",
      " 2.875      2.875      2.875      2.875      2.875      2.875\n",
      " 2.875      2.875      2.875      2.875      2.875      2.875\n",
      " 2.875      2.875      2.875      2.875      2.875      2.875\n",
      " 2.875      2.875      2.875      2.875      2.875      2.875\n",
      " 2.90625    2.91666667 2.9375     3.         3.         3.\n",
      " 3.         3.         3.         3.         3.         3.\n",
      " 3.         3.         3.         3.         3.         3.\n",
      " 3.         3.         3.         3.         3.         3.\n",
      " 3.         3.         3.         3.         3.         3.\n",
      " 3.         3.         3.         3.         3.         3.\n",
      " 3.         3.         3.         3.         3.         3.\n",
      " 3.         3.         3.         3.         3.         3.\n",
      " 3.         3.         3.         3.125      3.125      3.125\n",
      " 3.125      3.125      3.125      3.125      3.125      3.125\n",
      " 3.125      3.125      3.125      3.125      3.125      3.125\n",
      " 3.125      3.125      3.125      3.125      3.125      3.125\n",
      " 3.125      3.125      3.125      3.21875    3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.3125\n",
      " 3.3125     3.34375    3.375      3.375      3.375      3.375\n",
      " 3.375      3.375      3.375      3.375      3.375      3.375\n",
      " 3.375      3.375      3.375      3.375      3.41666667 3.41666667\n",
      " 3.45833333 3.45833333 3.5        3.5        3.5        3.5\n",
      " 3.5        3.5        3.5        3.5        3.5        3.5\n",
      " 3.5        3.5        3.5        3.5        3.5        3.5\n",
      " 3.5        3.5        3.5        3.5        3.5        3.5\n",
      " 3.5        3.5        3.5        3.5        3.5        3.5\n",
      " 3.5        3.5        3.5        3.5        3.5        3.5\n",
      " 3.5        3.5        3.5        3.5        3.5        3.5\n",
      " 3.5        3.5        3.5        3.5        3.59375    3.625\n",
      " 3.625      3.625      3.625      3.625      3.625      3.625\n",
      " 3.625      3.625      3.625      3.625      3.625      3.625\n",
      " 3.625      3.625      3.625      3.625      3.625      3.625\n",
      " 3.625      3.625      3.6875     3.6875     3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.875      3.875      3.875      3.875\n",
      " 3.875      3.875      3.875      3.875      3.875      3.875\n",
      " 3.875      3.875      3.875      3.875      3.875      3.875\n",
      " 3.875      3.875      3.875      3.875      3.875      3.875\n",
      " 3.9375     3.9375     4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.125      4.125\n",
      " 4.125      4.125      4.125      4.125      4.125      4.125\n",
      " 4.125      4.125      4.125      4.125      4.125      4.125\n",
      " 4.125      4.125      4.125      4.125      4.125      4.125\n",
      " 4.125      4.125      4.125      4.16666667 4.25       4.25\n",
      " 4.25       4.25       4.25       4.25       4.25       4.25\n",
      " 4.25       4.25       4.25       4.25       4.25       4.25\n",
      " 4.25       4.25       4.25       4.25       4.25       4.25\n",
      " 4.25       4.25       4.25       4.25       4.25       4.25\n",
      " 4.25       4.25       4.25       4.25       4.25       4.25\n",
      " 4.25       4.25       4.25       4.25       4.25       4.25\n",
      " 4.33333333 4.375      4.375      4.375      4.375      4.375\n",
      " 4.375      4.375      4.375      4.375      4.375      4.375\n",
      " 4.375      4.375      4.375      4.375      4.375      4.375\n",
      " 4.375      4.375      4.5        4.5        4.5        4.5\n",
      " 4.5        4.5        4.5        4.5        4.5        4.5\n",
      " 4.5        4.5        4.5        4.5        4.5        4.5\n",
      " 4.5        4.5        4.5        4.5        4.5        4.5\n",
      " 4.5        4.5        4.5        4.5        4.5        4.5\n",
      " 4.5        4.5        4.5        4.5        4.5        4.5\n",
      " 4.5        4.5        4.5        4.5        4.5        4.5\n",
      " 4.5        4.5        4.5        4.5        4.5        4.5\n",
      " 4.5        4.5        4.5        4.5        4.5        4.5\n",
      " 4.625      4.625      4.625      4.625     ]\n",
      "Missing values in X_train: 0\n",
      "Missing values in y_train: 0\n",
      "Training SVC...\n",
      "Training DecisionTreeClassifier...\n",
      "Training RandomForestClassifier...\n",
      "Training AdaBoostClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sridh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GaussianNB...\n",
      "Training GradientBoostingClassifier...\n",
      "Classifier: SVM\n",
      "Accuracy: 0.40707964601769914\n",
      "F1 Score: 0.3753058367758543\n",
      "ROC AUC: 0.7877274390467807\n",
      "\n",
      "\n",
      "Classifier: Decision Tree\n",
      "Accuracy: 0.36283185840707965\n",
      "F1 Score: 0.3599368564191973\n",
      "ROC AUC: 0.5766398781435266\n",
      "\n",
      "\n",
      "Classifier: Random Forest\n",
      "Accuracy: 0.45132743362831856\n",
      "F1 Score: 0.43971866521945435\n",
      "ROC AUC: 0.7596088258615284\n",
      "\n",
      "\n",
      "Classifier: AdaBoost\n",
      "Accuracy: 0.28761061946902655\n",
      "F1 Score: 0.28114902316435814\n",
      "ROC AUC: 0.573377318573513\n",
      "\n",
      "\n",
      "Classifier: Naive Bayes\n",
      "Accuracy: 0.35398230088495575\n",
      "F1 Score: 0.36538420243380343\n",
      "ROC AUC: 0.6601524990452755\n",
      "\n",
      "\n",
      "Classifier: GradientBoosting\n",
      "Accuracy: 0.4557522123893805\n",
      "F1 Score: 0.44370854888729433\n",
      "ROC AUC: 0.7087535847701139\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "# Fit and evaluate a classifier\n",
    "def evaluate_classifier(clf, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Fit the classifier and evaluate its performance.\"\"\"\n",
    "    print(f\"Training {clf.__class__.__name__}...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'F1 Score': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "    # Try to calculate ROC AUC if classifier supports predict_proba or decision_function\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        try:\n",
    "            y_prob = clf.predict_proba(X_test)\n",
    "            metrics['ROC AUC'] = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
    "        except ValueError:\n",
    "            metrics['ROC AUC'] = \"N/A - issue with probabilities\"\n",
    "    else:\n",
    "        metrics['ROC AUC'] = \"N/A - no predict_proba\"\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Compare different classifiers\n",
    "def compare_classifiers(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Compare multiple classifiers and tabulate their results.\"\"\"\n",
    "    classifiers = {\n",
    "        'SVM': SVC(probability=True),  # Ensure SVM has probability enabled\n",
    "        'Decision Tree': DecisionTreeClassifier(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'AdaBoost': AdaBoostClassifier(),\n",
    "        'Naive Bayes': GaussianNB(),\n",
    "        'GradientBoosting': GradientBoostingClassifier()\n",
    "    }\n",
    "\n",
    "    # Evaluate each classifier and store results\n",
    "    results = {}\n",
    "    for name, clf in classifiers.items():\n",
    "        results[name] = evaluate_classifier(clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Print the results\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"Classifier: {name}\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Main function for A3 (Classifier Comparison)\n",
    "# Load data\n",
    "data = pd.read_excel('training_mathbert 3.xlsx')\n",
    "X = data.iloc[:, :-1].values  # Features\n",
    "y = data.iloc[:, -1].values  # Target\n",
    "\n",
    "# Debugging: Check shapes and first few rows of data\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"X:\", X[:1000])\n",
    "print(\"y:\", y[:1000])\n",
    "\n",
    "# Ensure target labels are integers\n",
    "y = y.astype(int)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Debugging: Check for missing values\n",
    "print(f\"Missing values in X_train: {np.isnan(X_train).sum()}\")\n",
    "print(f\"Missing values in y_train: {np.isnan(y_train).sum()}\")\n",
    "\n",
    "# Compare classifiers\n",
    "compare_classifiers(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01823c8e-91d7-49f4-a14f-6a9714e67f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
